{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64f1672-3e04-4d8e-a298-19378ff0d202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\sanhith reddy\\.conda\\envs\\quora_env\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\sanhith reddy\\appdata\\roaming\\python\\python312\\site-packages (from lightgbm) (2.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\sanhith reddy\\.conda\\envs\\quora_env\\lib\\site-packages (from lightgbm) (1.16.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6844dd-bff8-4626-bb5a-0f401c24c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e2626d-2d5e-46c9-b458-a2b40bbc4ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 244)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len_x</th>\n",
       "      <th>q2_len_x</th>\n",
       "      <th>q1_num_words_x</th>\n",
       "      <th>q2_num_words_x</th>\n",
       "      <th>word_common_x</th>\n",
       "      <th>word_total_x</th>\n",
       "      <th>word_share_x</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>...</th>\n",
       "      <th>86_y</th>\n",
       "      <th>87_y</th>\n",
       "      <th>88_y</th>\n",
       "      <th>89_y</th>\n",
       "      <th>90_y</th>\n",
       "      <th>91_y</th>\n",
       "      <th>92_y</th>\n",
       "      <th>93_y</th>\n",
       "      <th>94_y</th>\n",
       "      <th>95_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>...</td>\n",
       "      <td>24.551529</td>\n",
       "      <td>-7.684039</td>\n",
       "      <td>-24.883157</td>\n",
       "      <td>6.526309</td>\n",
       "      <td>-24.260105</td>\n",
       "      <td>24.287874</td>\n",
       "      <td>23.207100</td>\n",
       "      <td>21.139578</td>\n",
       "      <td>-18.450893</td>\n",
       "      <td>36.307079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>...</td>\n",
       "      <td>24.103431</td>\n",
       "      <td>-11.546866</td>\n",
       "      <td>-62.848033</td>\n",
       "      <td>15.441650</td>\n",
       "      <td>-9.592031</td>\n",
       "      <td>16.754833</td>\n",
       "      <td>26.424757</td>\n",
       "      <td>28.875602</td>\n",
       "      <td>-23.002500</td>\n",
       "      <td>16.389976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>119</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.676087</td>\n",
       "      <td>0.289840</td>\n",
       "      <td>-21.327893</td>\n",
       "      <td>31.244260</td>\n",
       "      <td>-15.868776</td>\n",
       "      <td>10.563364</td>\n",
       "      <td>-9.109160</td>\n",
       "      <td>19.774184</td>\n",
       "      <td>-11.013100</td>\n",
       "      <td>28.307958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.053748</td>\n",
       "      <td>-5.903925</td>\n",
       "      <td>0.677170</td>\n",
       "      <td>-7.418554</td>\n",
       "      <td>-5.314642</td>\n",
       "      <td>-12.190648</td>\n",
       "      <td>-3.195340</td>\n",
       "      <td>8.207225</td>\n",
       "      <td>6.415468</td>\n",
       "      <td>9.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>...</td>\n",
       "      <td>7.919799</td>\n",
       "      <td>-5.115303</td>\n",
       "      <td>-11.169791</td>\n",
       "      <td>-4.767877</td>\n",
       "      <td>-16.489244</td>\n",
       "      <td>13.219784</td>\n",
       "      <td>0.407731</td>\n",
       "      <td>17.408289</td>\n",
       "      <td>-16.857232</td>\n",
       "      <td>22.797968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 244 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  is_duplicate  q1_len_x  q2_len_x  q1_num_words_x  q2_num_words_x  \\\n",
       "0   0             1        75        76              13              13   \n",
       "1   1             0        48        56              13              16   \n",
       "2   2             0       104       119              28              21   \n",
       "3   3             0        58       145              14              32   \n",
       "4   4             0        34        49               5               9   \n",
       "\n",
       "   word_common_x  word_total_x  word_share_x   cwc_min  ...       86_y  \\\n",
       "0             12            26          0.46  0.874989  ...  24.551529   \n",
       "1              8            24          0.33  0.666644  ...  24.103431   \n",
       "2              4            38          0.11  0.000000  ...  39.676087   \n",
       "3              1            34          0.03  0.000000  ...  11.053748   \n",
       "4              3            13          0.23  0.749981  ...   7.919799   \n",
       "\n",
       "        87_y       88_y       89_y       90_y       91_y       92_y  \\\n",
       "0  -7.684039 -24.883157   6.526309 -24.260105  24.287874  23.207100   \n",
       "1 -11.546866 -62.848033  15.441650  -9.592031  16.754833  26.424757   \n",
       "2   0.289840 -21.327893  31.244260 -15.868776  10.563364  -9.109160   \n",
       "3  -5.903925   0.677170  -7.418554  -5.314642 -12.190648  -3.195340   \n",
       "4  -5.115303 -11.169791  -4.767877 -16.489244  13.219784   0.407731   \n",
       "\n",
       "        93_y       94_y       95_y  \n",
       "0  21.139578 -18.450893  36.307079  \n",
       "1  28.875602 -23.002500  16.389976  \n",
       "2  19.774184 -11.013100  28.307958  \n",
       "3   8.207225   6.415468   9.979167  \n",
       "4  17.408289 -16.857232  22.797968  \n",
       "\n",
       "[5 rows x 244 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"final_features.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b6860d-3e11-433a-b39b-c98e31bc50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "y = df['is_duplicate']  \n",
    "\n",
    "# Features\n",
    "X = df.drop(columns=['id','is_duplicate'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e036b601-ca82-4736-9572-a11d56cdbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b7c16-3dba-4315-af81-84a32e02edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 300, 500],\n",
    "    \"max_depth\": [10, 15, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "# Use log_loss as scorer (minimize it â†’ better probability calibration)\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_log_loss\",  # sklearn wants a score â†’ negative log loss\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best Log Loss:\", -grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88303149-3886-478a-acf2-b5d12fe7cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Try isotonic calibration (works best if you have enough data)\n",
    "calibrated_rf = CalibratedClassifierCV(base_estimator=rf, method='isotonic', cv=5)\n",
    "calibrated_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_cal = calibrated_rf.predict(X_test)\n",
    "y_pred_proba_cal = calibrated_rf.predict_proba(X_test)\n",
    "\n",
    "print(\"\\nAfter Calibration:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_cal))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_cal))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_pred_proba_cal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2b239-1ae8-4320-8a4b-210739272f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Before calibration\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba[:, 1], n_bins=10)\n",
    "# After calibration\n",
    "prob_true_cal, prob_pred_cal = calibration_curve(y_test, y_pred_proba_cal[:, 1], n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(prob_pred, prob_true, \"o-\", label=\"Before Calibration\")\n",
    "plt.plot(prob_pred_cal, prob_true_cal, \"o-\", label=\"After Calibration\")\n",
    "plt.plot([0,1],[0,1], \"--\", color=\"gray\", label=\"Perfect Calibration\")\n",
    "\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"True probability\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a58f6-c251-43ca-9ae2-cb9c75996fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\" \n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "y_pred_proba = xgb.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be961f1-da1d-43f9-89a6-bacfed9ba6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANHITH REDDY\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m lr = LogisticRegression(max_iter=\u001b[32m2000\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, solver=\u001b[33m\"\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m lr.fit(X_train, y_train)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_pred = lr.predict(X_test)\n\u001b[32m      5\u001b[39m y_pred_proba = lr.predict_proba(X_test)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[33m\"\u001b[39m, accuracy_score(y_test, y_pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:375\u001b[39m, in \u001b[36mLinearClassifierMixin.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[33;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[32m    363\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m \u001b[33;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    374\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m scores = \u001b[38;5;28mself\u001b[39m.decision_function(X)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores.shape) == \u001b[32m1\u001b[39m:\n\u001b[32m    377\u001b[39m     indices = xp.astype(scores > \u001b[32m0\u001b[39m, indexing_dtype(xp))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:352\u001b[39m, in \u001b[36mLinearClassifierMixin.decision_function\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    349\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    350\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m X = validate_data(\u001b[38;5;28mself\u001b[39m, X, accept_sparse=\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, reset=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    353\u001b[39m scores = safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m.coef_.T, dense_output=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[38;5;28mself\u001b[39m.intercept_\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    355\u001b[39m     xp.reshape(scores, (-\u001b[32m1\u001b[39m,))\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (scores.ndim > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m scores.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m scores\n\u001b[32m    358\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = check_array(X, input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m, **check_params)\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     _assert_all_finite(\n\u001b[32m   1106\u001b[39m         array,\n\u001b[32m   1107\u001b[39m         input_name=input_name,\n\u001b[32m   1108\u001b[39m         estimator_name=estimator_name,\n\u001b[32m   1109\u001b[39m         allow_nan=ensure_all_finite == \u001b[33m\"\u001b[39m\u001b[33mallow-nan\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1110\u001b[39m     )\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m _assert_all_finite_element_wise(\n\u001b[32m    121\u001b[39m     X,\n\u001b[32m    122\u001b[39m     xp=xp,\n\u001b[32m    123\u001b[39m     allow_nan=allow_nan,\n\u001b[32m    124\u001b[39m     msg_dtype=msg_dtype,\n\u001b[32m    125\u001b[39m     estimator_name=estimator_name,\n\u001b[32m    126\u001b[39m     input_name=input_name,\n\u001b[32m    127\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=2000, n_jobs=-1, solver=\"saga\")\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_proba = lr.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_pred_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86385298-41f7-4baf-90d6-5f8a1b333329",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "y_pred_proba = lgb_model.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_pred_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be774518-1400-478a-999c-93bdb63c488d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd540d4-95d4-4418-8cae-3346c85fc594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quora_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
