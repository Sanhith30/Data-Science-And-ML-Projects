{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64f1672-3e04-4d8e-a298-19378ff0d202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\sanhith reddy\\.conda\\envs\\quora_env\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\sanhith reddy\\appdata\\roaming\\python\\python312\\site-packages (from lightgbm) (2.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\sanhith reddy\\.conda\\envs\\quora_env\\lib\\site-packages (from lightgbm) (1.16.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6844dd-bff8-4626-bb5a-0f401c24c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2e2626d-2d5e-46c9-b458-a2b40bbc4ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 1190)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>...</th>\n",
       "      <th>86_q2_y</th>\n",
       "      <th>87_q2_y</th>\n",
       "      <th>88_q2_y</th>\n",
       "      <th>89_q2_y</th>\n",
       "      <th>90_q2_y</th>\n",
       "      <th>91_q2_y</th>\n",
       "      <th>92_q2_y</th>\n",
       "      <th>93_q2_y</th>\n",
       "      <th>94_q2_y</th>\n",
       "      <th>95_q2_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34971</th>\n",
       "      <td>223062</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.199036</td>\n",
       "      <td>-27.237144</td>\n",
       "      <td>5.400709</td>\n",
       "      <td>28.044876</td>\n",
       "      <td>-30.063677</td>\n",
       "      <td>0.696971</td>\n",
       "      <td>39.786740</td>\n",
       "      <td>21.583338</td>\n",
       "      <td>-22.410946</td>\n",
       "      <td>5.302735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36569</th>\n",
       "      <td>121929</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545450</td>\n",
       "      <td>0.499996</td>\n",
       "      <td>0.374995</td>\n",
       "      <td>0.299997</td>\n",
       "      <td>0.428569</td>\n",
       "      <td>0.333332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.502255</td>\n",
       "      <td>0.144446</td>\n",
       "      <td>-3.402341</td>\n",
       "      <td>10.742955</td>\n",
       "      <td>-8.240209</td>\n",
       "      <td>3.727438</td>\n",
       "      <td>12.624058</td>\n",
       "      <td>9.990710</td>\n",
       "      <td>-2.459129</td>\n",
       "      <td>9.366579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30264</th>\n",
       "      <td>75717</td>\n",
       "      <td>0</td>\n",
       "      <td>0.499975</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.499987</td>\n",
       "      <td>0.499987</td>\n",
       "      <td>0.499992</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.411508</td>\n",
       "      <td>-10.531261</td>\n",
       "      <td>-11.455352</td>\n",
       "      <td>42.261574</td>\n",
       "      <td>-9.032182</td>\n",
       "      <td>27.496660</td>\n",
       "      <td>37.994663</td>\n",
       "      <td>25.499485</td>\n",
       "      <td>-21.955116</td>\n",
       "      <td>16.899961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9495</th>\n",
       "      <td>387155</td>\n",
       "      <td>1</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.799984</td>\n",
       "      <td>0.571420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.037388</td>\n",
       "      <td>-11.085898</td>\n",
       "      <td>-1.099428</td>\n",
       "      <td>-8.514374</td>\n",
       "      <td>-1.084490</td>\n",
       "      <td>-4.625764</td>\n",
       "      <td>5.879039</td>\n",
       "      <td>-5.895683</td>\n",
       "      <td>5.921881</td>\n",
       "      <td>7.653259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34432</th>\n",
       "      <td>77719</td>\n",
       "      <td>1</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.199996</td>\n",
       "      <td>0.499994</td>\n",
       "      <td>0.399996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.702414</td>\n",
       "      <td>-39.073620</td>\n",
       "      <td>1.129437</td>\n",
       "      <td>41.434350</td>\n",
       "      <td>17.061893</td>\n",
       "      <td>-11.556725</td>\n",
       "      <td>34.474068</td>\n",
       "      <td>40.167910</td>\n",
       "      <td>-44.360012</td>\n",
       "      <td>15.101011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  is_duplicate   cwc_min   cwc_max   csc_min   csc_max   ctc_min  \\\n",
       "34971  223062             0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "36569  121929             1  0.545450  0.499996  0.374995  0.299997  0.428569   \n",
       "30264   75717             0  0.499975  0.333322  0.499987  0.499987  0.499992   \n",
       "9495   387155             1  0.749981  0.749981  0.999900  0.333322  0.799984   \n",
       "34432   77719             1  0.749981  0.749981  0.333322  0.199996  0.499994   \n",
       "\n",
       "        ctc_max  last_word_eq  first_word_eq  ...    86_q2_y    87_q2_y  \\\n",
       "34971  0.000000           0.0            0.0  ...  17.199036 -27.237144   \n",
       "36569  0.333332           0.0            0.0  ...  11.502255   0.144446   \n",
       "30264  0.428565           0.0            0.0  ...  14.411508 -10.531261   \n",
       "9495   0.571420           0.0            1.0  ...  -5.037388 -11.085898   \n",
       "34432  0.399996           0.0            0.0  ...  16.702414 -39.073620   \n",
       "\n",
       "         88_q2_y    89_q2_y    90_q2_y    91_q2_y    92_q2_y    93_q2_y  \\\n",
       "34971   5.400709  28.044876 -30.063677   0.696971  39.786740  21.583338   \n",
       "36569  -3.402341  10.742955  -8.240209   3.727438  12.624058   9.990710   \n",
       "30264 -11.455352  42.261574  -9.032182  27.496660  37.994663  25.499485   \n",
       "9495   -1.099428  -8.514374  -1.084490  -4.625764   5.879039  -5.895683   \n",
       "34432   1.129437  41.434350  17.061893 -11.556725  34.474068  40.167910   \n",
       "\n",
       "         94_q2_y    95_q2_y  \n",
       "34971 -22.410946   5.302735  \n",
       "36569  -2.459129   9.366579  \n",
       "30264 -21.955116  16.899961  \n",
       "9495    5.921881   7.653259  \n",
       "34432 -44.360012  15.101011  \n",
       "\n",
       "[5 rows x 1190 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"final_features.csv\")\n",
    "df = df.sample(n=90000, random_state=2)\n",
    "print(df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7021bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'is_duplicate', 'cwc_min', 'cwc_max', 'csc_min', 'csc_max',\n",
      "       'ctc_min', 'ctc_max', 'last_word_eq', 'first_word_eq',\n",
      "       ...\n",
      "       '86_q2_y', '87_q2_y', '88_q2_y', '89_q2_y', '90_q2_y', '91_q2_y',\n",
      "       '92_q2_y', '93_q2_y', '94_q2_y', '95_q2_y'],\n",
      "      dtype='object', length=1190)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81b6860d-3e11-433a-b39b-c98e31bc50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "y = df['is_duplicate']  \n",
    "\n",
    "# Features\n",
    "X = df.drop(columns=['id','is_duplicate'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9f9da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90000,), (90000, 1188))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e036b601-ca82-4736-9572-a11d56cdbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec6e097f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72000, 1188), (18000, 1188), (72000,), (18000,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d27b7c16-3dba-4315-af81-84a32e02edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7122777777777778\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78     11369\n",
      "           1       0.62      0.58      0.60      6631\n",
      "\n",
      "    accuracy                           0.71     18000\n",
      "   macro avg       0.69      0.69      0.69     18000\n",
      "weighted avg       0.71      0.71      0.71     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88303149-3886-478a-acf2-b5d12fe7cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Log Loss: 0.5056944155574885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Predict probabilities for log loss\n",
    "y_proba_rf = rf.predict_proba(X_test)\n",
    "\n",
    "# Calculate log loss\n",
    "loss_rf = log_loss(y_test, y_proba_rf)\n",
    "print(\"Random Forest Log Loss:\", loss_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2b239-1ae8-4320-8a4b-210739272f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "280a58f6-c251-43ca-9ae2-cb9c75996fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7406666666666667\n",
      "F1 Score: 0.656259204712813\n",
      "Log Loss: 0.47438708835725657\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\" \n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "y_pred_proba = xgb.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "708c5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 2.4min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.6min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.6min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.6min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 4.2min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 3.6min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time= 3.4min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.5min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time= 2.4min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time= 1.6min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 1.6min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 1.5min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time= 1.4min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time= 2.3min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  31.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  30.9s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  32.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.7min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 1.5min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  48.6s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  48.3s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  49.5s\n",
      "Best Params: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None}\n",
      "Best CV Log Loss: 0.576241957256792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SANHITH REDDY\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1188 features, but RandomForestClassifier is expecting 138 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m best_model = rand_search.best_estimator_\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m y_pred = best_model.predict(X_test)\n\u001b[32m     49\u001b[39m y_pred_proba = best_model.predict_proba(X_test)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SANHITH REDDY\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:903\u001b[39m, in \u001b[36mForestClassifier.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    883\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m    Predict class for X.\u001b[39;00m\n\u001b[32m    885\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    901\u001b[39m \u001b[33;03m        The predicted classes.\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m     proba = \u001b[38;5;28mself\u001b[39m.predict_proba(X)\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m:\n\u001b[32m    906\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classes_.take(np.argmax(proba, axis=\u001b[32m1\u001b[39m), axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SANHITH REDDY\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:945\u001b[39m, in \u001b[36mForestClassifier.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    943\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m945\u001b[39m X = \u001b[38;5;28mself\u001b[39m._validate_X_predict(X)\n\u001b[32m    947\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m    948\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SANHITH REDDY\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = validate_data(\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    639\u001b[39m     X,\n\u001b[32m    640\u001b[39m     dtype=DTYPE,\n\u001b[32m    641\u001b[39m     accept_sparse=\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    642\u001b[39m     reset=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    643\u001b[39m     ensure_all_finite=ensure_all_finite,\n\u001b[32m    644\u001b[39m )\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SANHITH REDDY\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     _check_n_features(_estimator, X, reset=reset)\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SANHITH REDDY\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 1188 features, but RandomForestClassifier is expecting 138 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],         # keep small range\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,                 # reduce number of trials to save memory\n",
    "    scoring=\"neg_log_loss\",    # optimize log loss\n",
    "    cv=3,                      # 3-fold CV\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=1                   # use 1 job to avoid memory blow-up\n",
    ")\n",
    "\n",
    "rand_search.fit(X_train, y_train)\n",
    "print(\"Best Params:\", rand_search.best_params_)\n",
    "print(\"Best CV Log Loss:\", -rand_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = rand_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# Metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Test Log Loss:\", log_loss(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be961f1-da1d-43f9-89a6-bacfed9ba6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANHITH REDDY\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m lr = LogisticRegression(max_iter=\u001b[32m2000\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, solver=\u001b[33m\"\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m lr.fit(X_train, y_train)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_pred = lr.predict(X_test)\n\u001b[32m      5\u001b[39m y_pred_proba = lr.predict_proba(X_test)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[33m\"\u001b[39m, accuracy_score(y_test, y_pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:375\u001b[39m, in \u001b[36mLinearClassifierMixin.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[33;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[32m    363\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m \u001b[33;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    374\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m scores = \u001b[38;5;28mself\u001b[39m.decision_function(X)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores.shape) == \u001b[32m1\u001b[39m:\n\u001b[32m    377\u001b[39m     indices = xp.astype(scores > \u001b[32m0\u001b[39m, indexing_dtype(xp))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:352\u001b[39m, in \u001b[36mLinearClassifierMixin.decision_function\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    349\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    350\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m X = validate_data(\u001b[38;5;28mself\u001b[39m, X, accept_sparse=\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, reset=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    353\u001b[39m scores = safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m.coef_.T, dense_output=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[38;5;28mself\u001b[39m.intercept_\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    355\u001b[39m     xp.reshape(scores, (-\u001b[32m1\u001b[39m,))\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (scores.ndim > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m scores.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m scores\n\u001b[32m    358\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = check_array(X, input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m, **check_params)\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     _assert_all_finite(\n\u001b[32m   1106\u001b[39m         array,\n\u001b[32m   1107\u001b[39m         input_name=input_name,\n\u001b[32m   1108\u001b[39m         estimator_name=estimator_name,\n\u001b[32m   1109\u001b[39m         allow_nan=ensure_all_finite == \u001b[33m\"\u001b[39m\u001b[33mallow-nan\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1110\u001b[39m     )\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m _assert_all_finite_element_wise(\n\u001b[32m    121\u001b[39m     X,\n\u001b[32m    122\u001b[39m     xp=xp,\n\u001b[32m    123\u001b[39m     allow_nan=allow_nan,\n\u001b[32m    124\u001b[39m     msg_dtype=msg_dtype,\n\u001b[32m    125\u001b[39m     estimator_name=estimator_name,\n\u001b[32m    126\u001b[39m     input_name=input_name,\n\u001b[32m    127\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\quora_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=2000, n_jobs=-1, solver=\"saga\")\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_proba = lr.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_pred_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86385298-41f7-4baf-90d6-5f8a1b333329",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "y_pred_proba = lgb_model.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_pred_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be774518-1400-478a-999c-93bdb63c488d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd540d4-95d4-4418-8cae-3346c85fc594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quora_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
